{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 單天版 -- 將網頁寫入txt裡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "with open('ltn.txt', 'w') as f:\n",
    "    url = 'http://news.ltn.com.tw/newspaper/focus/20160310'\n",
    "    res = requests.get(url)\n",
    "    soup = bs(res.text)\n",
    "\n",
    "    Times = 'http://news.ltn.com.tw'\n",
    "    sort = soup.select('.newspaper a')\n",
    "    for a in xrange(0, 12):\n",
    "        url2 = Times + sort[a]['href'] #分類網址\n",
    "        res2 = requests.get(url2)\n",
    "        soup2 = bs(res2.text)\n",
    "        print len(soup2.select('.p_num'))\n",
    "        if len(soup2.select('.p_num')) == 1 : #判斷此類別新聞是否一頁\n",
    "            title = soup2.select('.picword')\n",
    "            title_num = len(soup2.select('.picword')) #計算有幾篇文章(每天新聞數量不同)\n",
    "            for i in xrange(0 , title_num):   \n",
    "                url1=Times+title[i]['href']   #文章網址\n",
    "                f.write(url1  + '\\n')  #寫入text\n",
    "        else:\n",
    "            for page1 in xrange (1,len(soup2.select('.p_num'))+1): #此類別新聞超過一頁時執行\n",
    "                url3= url2+'?page={}'.format(page1)\n",
    "                print url3\n",
    "                res3 = requests.get(url3)\n",
    "                soup3 = bs(res3.text)\n",
    "                title = soup3.select('.picword')\n",
    "                title_num1 = len(soup3.select('.picword')) #計算有幾篇文章(每天新聞數量不同)\n",
    "                for i in xrange(0 , title_num1):   \n",
    "                    url1=Times+title[i]['href']   #文章網址\n",
    "                    f.write(url1  + '\\n')  #寫入text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 單天版-- 讀取txt裡網頁 並寫成檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8e2bd564853b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;34m'opinion'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mwr_talk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[1;31m#print x[0],x[1],x[2]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-8e2bd564853b>\u001b[0m in \u001b[0;36mwr_talk\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdiv3\u001b[0m \u001b[1;32min\u001b[0m \u001b[0messay\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0messay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mpage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdiv3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m       \u001b[1;31m#加入內文\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtital\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import sys\n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding(\"utf-8\")   #解決UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-6:ordinal not in range(128) \n",
    "\n",
    "def wr_all(url):  #方法 大部分的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')\n",
    "    date =  soup.select('#newstext span ')\n",
    "    category = soup.select('.guide  a ')\n",
    "    keyword= soup.select('.con_keyword')\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文       \n",
    "    return tital[0].text,date[0].text,category[1].text, page\n",
    "        \n",
    "def wr_sport(url):  #方法 運動類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)    \n",
    "    essay = soup.select('h4 , p')\n",
    "    tital = soup.select('.Btitle ')\n",
    "    date =  soup.select('.news_content .date ')\n",
    "    category = \"體育\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)     \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "     \n",
    "def wr_ent(url):  #方法 娛樂類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('p')\n",
    "    tital = soup.select('.news_content h1 ')\n",
    "    date =  soup.select('.news_content .date')\n",
    "    category = \"娛樂\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)    \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "\n",
    "def wr_local(url): #方法 地方類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')\n",
    "    date =  soup.select('#newstext span ')\n",
    "    category1 = \"地方\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)\n",
    "    return tital[0].text,date[0].text,category1, page\n",
    "    \n",
    "def wr_talk(url): # 方法 言論類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('.cont p')\n",
    "    tital = soup.select('h2 ')\n",
    "    date =  soup.select('.writer span')\n",
    "    category = \"言論\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)    \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "        \n",
    "    \n",
    "        \n",
    "with open('ltn.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        url = line.strip()\n",
    "        m = re.match('.*news/(\\w+)/paper/(\\w+)' ,url)  #正規表達法 切出m.group(1)類別 與 m.group(2)網頁最後的數字(用來當檔名)\n",
    "        print m.group(1)\n",
    "        if m.group(1) == 'sports': #判斷是否為體育類\n",
    "            x=  wr_sport(url) #使用方法\n",
    "            #print x[0],x[1],x[2]\n",
    "            name =  m.group(2)\n",
    "            f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            iii_num = len(x)\n",
    "            for iii in xrange (0, iii_num):\n",
    "                f.write( x[iii] + '\\n') #寫入內文      \n",
    "            f.close()\n",
    "            time.sleep(1)\n",
    "            \n",
    "        elif m.group(1) == 'entertainment': #判斷是否為娛樂類\n",
    "            x= wr_ent(url) #使用方法\n",
    "            #print x[0],x[1],x[2]\n",
    "            name =  m.group(2)\n",
    "            f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            iii_num = len(x) #計算陣列長度\n",
    "            for iii in xrange (0, iii_num):\n",
    "                f.write( x[iii] + '\\n') #寫入內文           \n",
    "            f.close()\n",
    "            time.sleep(1)\n",
    "            \n",
    "        elif m.group(1) =='local': #判斷是否為地方類\n",
    "            x= wr_local(url) #使用方法\n",
    "            #print x[0],x[1],x[2]\n",
    "            name =  m.group(2)\n",
    "            f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            iii_num = len(x)  \n",
    "            for iii in xrange (0, iii_num):\n",
    "                f.write( x[iii] + '\\n') #寫入內文         \n",
    "            f.close()\n",
    "            time.sleep(1)\n",
    "            \n",
    "        elif m.group(1) =='opinion': #判斷是否為言論類\n",
    "            x= wr_talk(url) #使用方法\n",
    "            #print x[0],x[1],x[2]\n",
    "            name =  m.group(2)\n",
    "            f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            iii_num = len(x)\n",
    "            for iii in xrange (0, iii_num):\n",
    "                f.write( x[iii] + '\\n') #寫入內文          \n",
    "            f.close()\n",
    "            time.sleep(1)\n",
    "            \n",
    "        else:\n",
    "            x= wr_all(url) #使用方法\n",
    "            #print x[0],x[1],x[2]\n",
    "            name =  m.group(2)\n",
    "            f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            iii_num = len(x)\n",
    "            for iii in xrange (0, iii_num):\n",
    "                f.write( x[iii] + '\\n') #寫入內文        \n",
    "            f.close()\n",
    "            time.sleep(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全年無休版--將網頁寫入txt裡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import datetime as dt\n",
    "#製造日期的迴圈\n",
    "startdate = dt.datetime(2016, 3,9)  #輸入開始日期\n",
    "endate = dt.datetime(2016, 3,12)       #輸入結束日期\n",
    "totaldays = (endate - startdate).days + 1  #計算有幾天\n",
    "for daynumber in range(totaldays):\n",
    "    datestring = (startdate + dt.timedelta(days = daynumber)).date().strftime(\"%Y%m%d\")  \n",
    "    \n",
    "    with open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(datestring), 'w') as f:\n",
    "\n",
    "        url = 'http://news.ltn.com.tw/newspaper/focus/{}'.format(datestring)\n",
    "        res = requests.get(url)\n",
    "        soup = bs(res.text)\n",
    "\n",
    "        Times = 'http://news.ltn.com.tw'\n",
    "        sort = soup.select('.newspaper a')\n",
    "        for a in xrange(0, 12):\n",
    "            url2 = Times + sort[a]['href'] #分類網址\n",
    "            res2 = requests.get(url2)\n",
    "            soup2 = bs(res2.text)\n",
    "            print len(soup2.select('.p_num'))\n",
    "            if len(soup2.select('.p_num')) == 1 : #判斷此類別新聞是否一頁\n",
    "                title = soup2.select('.picword')\n",
    "                title_num = len(soup2.select('.picword')) #計算有幾篇文章(每天新聞數量不同)\n",
    "                for i in xrange(0 , title_num):   \n",
    "                    url1=Times+title[i]['href']   #文章網址\n",
    "                    f.write(url1  + '\\n')  #寫入text\n",
    "            else:\n",
    "                for page1 in xrange (1,len(soup2.select('.p_num'))+1): #此類別新聞超過一頁時執行\n",
    "                    url3= url2+'?page={}'.format(page1)\n",
    "                    print url3\n",
    "                    res3 = requests.get(url3)\n",
    "                    soup3 = bs(res3.text)\n",
    "                    title = soup3.select('.picword')\n",
    "                    title_num1 = len(soup3.select('.picword')) #計算有幾篇文章(每天新聞數量不同)\n",
    "                    for i in xrange(0 , title_num1):   \n",
    "                        url1=Times+title[i]['href']   #文章網址\n",
    "                        f.write(url1  + '\\n')  #寫入text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全年無休版--讀取txt裡網頁 並寫成檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ab1b4342da1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    146\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miii\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#寫入內文\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding(\"utf-8\")   #解決UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-6:ordinal not in range(128) \n",
    "\n",
    "\n",
    "\n",
    "def wr_all(url):  #方法 大部分的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')\n",
    "    date =  soup.select('#newstext span ')\n",
    "    category = soup.select('.guide  a ')\n",
    "    keyword= soup.select('.con_keyword')\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文       \n",
    "    return tital[0].text,date[0].text,category[1].text, page\n",
    "        \n",
    "def wr_sport(url):  #方法 運動類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)    \n",
    "    essay = soup.select('h4 , p')\n",
    "    tital = soup.select('.Btitle ')\n",
    "    date =  soup.select('.news_content .date ')\n",
    "    category = \"體育\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)     \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "     \n",
    "def wr_ent(url):  #方法 娛樂類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('p')\n",
    "    tital = soup.select('.news_content h1 ')\n",
    "    date =  soup.select('.news_content .date')\n",
    "    category = \"娛樂\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)    \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "\n",
    "def wr_local(url): #方法 地方類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')\n",
    "    date =  soup.select('#newstext span ')\n",
    "    category1 = \"地方\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)\n",
    "    return tital[0].text,date[0].text,category1, page\n",
    "    \n",
    "def wr_talk(url): # 方法 言論類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('.cont p')\n",
    "    tital = soup.select('h2 ')\n",
    "    date =  soup.select('.writer span')\n",
    "    category = \"言論\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)    \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "        \n",
    "    \n",
    "startdate = dt.datetime(2016, 3,9)  #輸入開始日期\n",
    "endate = dt.datetime(2016, 3,12)       #輸入結束日期\n",
    "totaldays = (endate - startdate).days + 1  #計算有幾天\n",
    "for daynumber in range(totaldays):\n",
    "    datestring = (startdate + dt.timedelta(days = daynumber)).date().strftime(\"%Y%m%d\")  \n",
    "        \n",
    "    with open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(datestring), 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            url = line.strip()\n",
    "            m = re.match('.*news/(\\w+)/paper/(\\w+)' ,url)  #正規表達法 切出m.group(1)類別 與 m.group(2)網頁最後的數字(用來當檔名)\n",
    "            print m.group(1)\n",
    "            if m.group(1) == 'sports': #判斷是否為體育類\n",
    "                x=  wr_sport(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                name =  m.group(2)\n",
    "                f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "                iii_num = len(x)\n",
    "                for iii in xrange (0, iii_num):\n",
    "                    f.write( x[iii] + '\\n') #寫入內文      \n",
    "                f.close()\n",
    "                time.sleep(1)\n",
    "\n",
    "            elif m.group(1) == 'entertainment': #判斷是否為娛樂類\n",
    "                x= wr_ent(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                name =  m.group(2)\n",
    "                f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "                iii_num = len(x) #計算陣列長度\n",
    "                for iii in xrange (0, iii_num):\n",
    "                    f.write( x[iii] + '\\n') #寫入內文           \n",
    "                f.close()\n",
    "                time.sleep(1)\n",
    "\n",
    "            elif m.group(1) =='local': #判斷是否為地方類\n",
    "                x= wr_local(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                name =  m.group(2)\n",
    "                f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "                iii_num = len(x)  \n",
    "                for iii in xrange (0, iii_num):\n",
    "                    f.write( x[iii] + '\\n') #寫入內文         \n",
    "                f.close()\n",
    "                time.sleep(1)\n",
    "\n",
    "            elif m.group(1) =='opinion': #判斷是否為言論類\n",
    "                x= wr_talk(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                name =  m.group(2)\n",
    "                f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "                iii_num = len(x)\n",
    "                for iii in xrange (0, iii_num):\n",
    "                    f.write( x[iii] + '\\n') #寫入內文          \n",
    "                f.close()\n",
    "                time.sleep(1)\n",
    "\n",
    "            else:\n",
    "                x= wr_all(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                name =  m.group(2)\n",
    "                f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "                iii_num = len(x)\n",
    "                for iii in xrange (0, iii_num):\n",
    "                    f.write( x[iii] + '\\n') #寫入內文        \n",
    "                f.close()\n",
    "                time.sleep(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
