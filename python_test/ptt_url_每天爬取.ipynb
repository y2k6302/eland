{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設計日期迴圈\n",
    "- PTT網頁版1頁有20篇文章\n",
    "- PTT會固定刪文章，約停在15000頁\n",
    "- 約前兩天的文章會留下最多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天日期 =>2016-09-19\n"
     ]
    }
   ],
   "source": [
    "# #抓日期\n",
    "# import time\n",
    "# import datetime\n",
    "# #今天日期 \n",
    "# today = datetime.date.today()\n",
    "# print \"今天日期 =>\" + str(today)\n",
    "#設定要相減的日期\n",
    "# other_day = datetime.date(2016,9,6)\n",
    "# result = today - other_day\n",
    "# print result\n",
    "# #如果只想要把相差的天數拿出來的話\n",
    "# day = result.days\n",
    "# print \"相減後的天數 =>\" + str(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 設定抓取頁數\n",
    "- 先抓到總頁數\n",
    "- 往前推150頁抓取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#coding=utf-8 \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#通過18禁頁面\n",
    "url = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "playload = {\n",
    "    'from':'/bbs/Gossiping/index.html',\n",
    "    'yes':'yes'\n",
    "}\n",
    "rs = requests.session() #設定一個session\n",
    "res1 = rs.post('https://www.ptt.cc/ask/over18',verify=False,data = playload) #透過session記錄通過18禁頁面\n",
    "res = rs.get(url,verify=False)\n",
    "soup = BeautifulSoup(res.text)\n",
    "temp_page_num = soup.select('.btn.wide')[1]['href'].split('index')[1].split('.')[0]\n",
    "page_num = int(temp_page_num)+1\n",
    "# print '總頁數'+str(page_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14912\n",
      "15062\n"
     ]
    }
   ],
   "source": [
    "#每天要爬取多少頁數\n",
    "num = 150\n",
    "\n",
    "#啟始頁面index\n",
    "starIndex = page_num-num\n",
    "\n",
    "#結束頁面index\n",
    "endIndex = starIndex + num\n",
    "\n",
    "print starIndex\n",
    "print endIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 爬取前一天的PTT文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160925\n"
     ]
    }
   ],
   "source": [
    "#抓日期\n",
    "import time\n",
    "import datetime\n",
    "today = datetime.datetime.now()\n",
    "#前一天的日期\n",
    "yestoday = today + datetime.timedelta(days = -1)\n",
    "yestoday = yestoday.strftime('%Y%m%d')\n",
    "fileName = yestoday\n",
    "year =  today.strftime('%Y')\n",
    "print yestoday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 爬取每一頁的文章url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urlparse import urljoin\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "#主要網址\n",
    "main_url = 'https://www.ptt.cc/bbs/Gossiping/'\n",
    "errorCount = 0\n",
    "error_url = ''\n",
    "\n",
    "#要印出的 url 長串\n",
    "urlString = ''\n",
    "\n",
    "#相關資料\n",
    "playload = {\n",
    "    'from':'/bbs/Gossiping/index.html',\n",
    "    'yes':'yes'\n",
    "}\n",
    "#設定一個session\n",
    "rs = requests.session()\n",
    "\n",
    "#先session記錄通過18禁頁面\n",
    "res1 = rs.post('https://www.ptt.cc/ask/over18',verify=False,data = playload)\n",
    "\n",
    "\n",
    "#開始爬文\n",
    "for n in range(starIndex,endIndex):\n",
    "    url = 'https://www.ptt.cc/bbs/Gossiping/index{}.html'.format(n)\n",
    "\n",
    "    #進入爬取內文\n",
    "    res = rs.get(url,verify=False)\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    try:\n",
    "        for entry in soup.select('.r-ent'):\n",
    "        #     print len(entry.select('.title a'))\n",
    "\n",
    "            #判斷該文章是否已被刪除，刪除即\"跳過\"\n",
    "            if len(entry.select('.title a ')) != 0 :\n",
    "\n",
    "                #爬取資訊\n",
    "                nrec = entry.select('.nrec')[0].text.strip()\n",
    "                if nrec=='':\n",
    "                    nrec='0'\n",
    "                title = entry.select('.title')[0].text.strip()\n",
    "                category = category = title.split('[')[1].split(']')[0].strip()\n",
    "                author = entry.select('.author')[0].text.strip()\n",
    "                href =  entry.select('a')[0]['href']\n",
    "                article_url = urljoin(main_url,href) #urljoin會自動合併兩個網址\n",
    "\n",
    "                #設定日期\n",
    "                date = entry.select('.date')[0].text.strip()\n",
    "                month,day = date.split('/')[0].strip(),date.split('/')[1].strip()\n",
    "                if int(month) < 10:\n",
    "                    month = '0'+month\n",
    "                newDate = year+month+day\n",
    "                \n",
    "                #文章url為前兩天日期，才寫入file\n",
    "                date_time_1 = datetime.datetime.strptime(newDate,'%Y%m%d')\n",
    "                date_time_2 = datetime.datetime.strptime(yestoday,'%Y%m%d')\n",
    "                time.sleep(1)\n",
    "                \n",
    "                if date_time_1 == date_time_2:\n",
    "                    urlString += article_url+','+category+','+nrec+','+title+','+newDate+','+author+'\\n'\n",
    "\n",
    "        #         print article_url,title,category,newDate,author,nrec\n",
    "        time.sleep(4)\n",
    "    except:\n",
    "        errorCount += 1\n",
    "        error_url += article_url+','\n",
    "        continue\n",
    "# print urlString\n",
    "f = open('C:/Users/ytchen/Desktop/ptt/{}.txt'.format(fileName),'w')\n",
    "f.write(urlString.encode('utf-8')) #最後再一次寫出來\n",
    "f.close()   \n",
    "\n",
    "#log記錄\n",
    "logString = ''\n",
    "fileName_log = yestoday+'/'+str(starIndex)+'/'+str(endIndex)+'/'+'error:'+str(errorCount)+'\\n'+'error_url:'+error_url+'\\n'\n",
    "f1 = open('C:/Users/ytchen/Desktop/ptt/log.txt','r')\n",
    "for logLine in f1.readlines():\n",
    "    logString += logLine\n",
    "f1.close()\n",
    "\n",
    "f2 = open('C:/Users/ytchen/Desktop/ptt/log.txt','w')\n",
    "logString += fileName_log\n",
    "f2.write(logString.encode('utf-8'))\n",
    "f2.close() "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
