{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:54: UnicodeWarning: Unicode unequal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "#把分類跟 url 組合在一起\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urlparse import urljoin\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "urls = \"\" #裝要印出的所有url\n",
    "main_url ='http://www.chinatimes.com'\n",
    "\n",
    "#時間\n",
    "today = datetime.datetime.now()\n",
    "yestoday = today + datetime.timedelta(days = -1)\n",
    "yestoday = yestoday.strftime('%Y-%m-%d')\n",
    "fileName = yestoday.replace('-','')\n",
    "\n",
    "#target url\n",
    "page_url = 'http://www.chinatimes.com/history-by-date/{}-2601'.format(yestoday)\n",
    "\n",
    "#request submit\n",
    "res = requests.get(page_url)\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "#今天有多少篇新聞\n",
    "div = soup.select('.listLeft li')[0]\n",
    "totalNewsNum = div.select('span')[1].text.replace('(','').replace(')','')\n",
    "\n",
    "#無條件進位，算出總共會有幾頁新聞\n",
    "totalPageNum = int(math.ceil(int(totalNewsNum) / 11))\n",
    "# print(totalPageNum)\n",
    "\n",
    "#爬取每一頁的11篇新聞\n",
    "for n in range(1,totalPageNum+1):\n",
    "    #target url\n",
    "    page_url = 'http://www.chinatimes.com/history-by-date/{}-2601?page={}'.format(yestoday,n)\n",
    "    \n",
    "    #request submit\n",
    "    res = requests.get(page_url)\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    \n",
    "    #抓到每一頁11篇新聞的url\n",
    "    lis = soup.select('.listRight li')\n",
    "    \n",
    "    #sleep\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for li in lis:\n",
    "        href = li.select('a')[0]['href']\n",
    "        category = li.select('a')[1].text.strip()\n",
    "        news_url = urljoin(main_url,href)+'\\n' #urljoin會自動合併兩個網址\n",
    "\n",
    "        #進行新聞的篩選\n",
    "        if(category != '藝文副刊' and category != '《旺來報》' and category != '時尚消費'):\n",
    "            urls += category + news_url  #把新聞串在一起等著印出\n",
    "#             print(category)\n",
    "#             print(news_url)       \n",
    "f = open('C:/Users/ytchen/Desktop/ETtoday/{}.txt'.format(fileName),'w')\n",
    "f.write(urls.encode('utf-8')) #最後再一次寫出來\n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將新聞爬取下來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#這個才是最正確的\n",
    "#修改成一次爬好幾個月\n",
    "import string\n",
    "import datetime as dt\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "#全域變數\n",
    "error_count = 0\n",
    "error_url = ''\n",
    "all_json_str = [] #存所有資訊的 json\n",
    "\n",
    "#抓日期\n",
    "import datetime\n",
    "today = datetime.datetime.now()\n",
    "yestoday = today + datetime.timedelta(days = -1)\n",
    "yestoday = yestoday.strftime('%Y-%m-%d')\n",
    "fileName = yestoday.replace('-','')\n",
    "\n",
    "#開起每天的url檔案\n",
    "f = open('C:/Users/ytchen/Desktop/ETtoday/{}.txt'.format(fileName),'r')\n",
    "for url_no in f.readlines():\n",
    "    \n",
    "    try:\n",
    "        #抓類別\n",
    "        category = url_no.split('http')[0]\n",
    "\n",
    "        #因為text檔每行後面有換行符號\\n，必須要去除掉，否則會沒辦法送出request\n",
    "        url = 'http'+ url_no.split('http')[1].replace('\\n','')   #重新組合url   \n",
    "        res = requests.get(url) #利用get發送請求\n",
    "        soup = BeautifulSoup(res.text)\n",
    "\n",
    "        #抓標題跟副標題\n",
    "        article = soup.select('article')[0]\n",
    "        title = article.select('h1')[0].text\n",
    "        # print(title)\n",
    "\n",
    "        #點閱率\n",
    "        div = soup.select('.art_click.clear-fix')[0]\n",
    "        click = div.select('span')[1].text\n",
    "        # print(click)\n",
    "\n",
    "        #抓內文\n",
    "        article = ''\n",
    "        div= soup.select('article')[0]   #先把所有內文抓到\n",
    "        ps = div.select('p')                #再把裡面所有的p標籤抓到\n",
    "        for p in ps:                        #把每個p標籤一個一個抓出來印\n",
    "            article += p.text.replace('\\r','').replace('\\n','')  #段落會隱藏換行符號\\r，要取代掉，不然後面讀取json格式會只讀到最後一行\n",
    "        # print(article)\n",
    "\n",
    "        #抓keyword\n",
    "        keyNum = len(soup.select('.a_k a'))\n",
    "        allKeyword = ''\n",
    "        for n in range(0,keyNum):\n",
    "            keyword = soup.select('.a_k a')[n].text\n",
    "            allKeyword += keyword +' '\n",
    "        allKeyword = allKeyword.strip()\n",
    "        # print(allKeyword)\n",
    "\n",
    "        #將資訊存成 json檔\n",
    "        data  =  { \n",
    "            'comp':'ettoday',  #新聞來源\n",
    "            'category':category, #類別\n",
    "            'title':title, #標題\n",
    "            'content':article, #內文\n",
    "            'date':fileName,  #直接抓檔案的日期來用\n",
    "            'url':url,\n",
    "            'keyw':allKeyword,\n",
    "            'click':click\n",
    "        }\n",
    "\n",
    "        #一篇新聞一本字典\n",
    "        all_json_str.append(data) \n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        error_count +=1\n",
    "        error_url += url+','\n",
    "        continue         \n",
    "#檔案儲存\n",
    "f1 = open('C:/Users/ytchen/Desktop/ETtoday/json/{}.json'.format(fileName),'w')\n",
    "b =json.dumps(all_json_str) #再把 list 字典轉換成 json 並包起來\n",
    "f1.write(b)\n",
    "\n",
    "#寫入檔案結束\n",
    "f1.close()\n",
    "f.close()\n",
    "\n",
    "#log記錄\n",
    "logString = ''\n",
    "fileName_log = fileName+'/'+'errorCount:'+str(error_count)+'\\n'+'error_url:'+error_url+'\\n'\n",
    "#讀取\n",
    "with open('C:/Users/ytchen/Desktop/ETtoday/log_content.txt','r') as f1:\n",
    "    for logLine in f1.readlines():\n",
    "        logString += logLine\n",
    "#重新寫入\n",
    "with open('C:/Users/ytchen/Desktop/ETtoday/log_content.txt','w') as f2:\n",
    "    logString += fileName_log\n",
    "    f2.write(logString.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將json倒入MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import datetime\n",
    "today = datetime.datetime.now()\n",
    "yestoday = today + datetime.timedelta(days = -1)\n",
    "yestoday = yestoday.strftime('%Y-%m-%d')\n",
    "fileName = yestoday.replace('-','')\n",
    "# fileName = '20160917'\n",
    "\n",
    "f = open('C:/Users/ytchen/Desktop/ETtoday/json/{}.json'.format(fileName),'r')\n",
    "i = f.read()\n",
    "json = json.loads(i)\n",
    "print(len(json))\n",
    "    \n",
    "client = MongoClient() \n",
    "database = client[\"test\"]\n",
    "collection = database[\"news\"]\n",
    "collection.insert_many(\n",
    "    json\n",
    ")\n",
    "client.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
