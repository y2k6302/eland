{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抓取前一天日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20161015\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "today = datetime.datetime.now()\n",
    "yestoday = today + datetime.timedelta(days = -1)\n",
    "print(yestoday.strftime('%Y%m%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存取url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "#抓日期\n",
    "import datetime\n",
    "today = datetime.datetime.now()\n",
    "yestoday = today + datetime.timedelta(days = -1)\n",
    "yestoday = yestoday.strftime('%Y%m%d')\n",
    "file_datestring = yestoday\n",
    "#把分類跟 url 組合在一起\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urlparse import urljoin\n",
    "import os\n",
    "import datetime as dt\n",
    "import time\n",
    "urls = \"\" #裝要印出的所有url\n",
    "setList = [] #裝不重複的url\n",
    "\n",
    "#抓到昨天蘋果新聞的url\n",
    "page_url = 'http://www.appledaily.com.tw/appledaily/archive/{}'.format(yestoday)\n",
    "\n",
    "# print(page_url)\n",
    "main_url ='http://www.appledaily.com.tw/appledaily'\n",
    "res = requests.get(page_url)\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "#包含所有種類的div\n",
    "div = soup.select('.abdominis.clearmen')[0]\n",
    "\n",
    "#抓取所有連結\n",
    "lis = div.select('.nclns li')\n",
    "for li in lis:\n",
    "    href = li.select('a')[0]['href']\n",
    "    category = href.split('/')[3]\n",
    "    if(category=='enews'):\n",
    "        category='entertainment'\n",
    "    news_url = urljoin(main_url,href)+\"\\n\" #urljoin會自動合併兩個網址\n",
    "    \n",
    "    #進行新聞的篩選\n",
    "    if(category != 'supplement' and category != 'article' and category != 'forum' and category !='adcontent'):\n",
    "        urls += category + news_url  #把新聞串在一起等著印出\n",
    "time.sleep(0.5)\n",
    "f = open('C:/Users/ytchen/Desktop/AppleNews/{}.txt'.format(file_datestring),'w')\n",
    "f.write(urls.encode('utf-8')) #最後再一次寫出來\n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將檔案寫入file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#這個才是最正確的\n",
    "#修改成一次爬好幾個月\n",
    "import string\n",
    "import datetime as dt\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "#全域變數\n",
    "error_count = 0\n",
    "error_url = ''\n",
    "\n",
    "#抓日期\n",
    "import datetime\n",
    "today = datetime.datetime.now()\n",
    "yestoday = today + datetime.timedelta(days = -1)\n",
    "yestoday = yestoday.strftime('%Y%m%d')\n",
    "file_datestring = yestoday\n",
    "\n",
    "#準備抓資料\n",
    "all_json_str = [] #存所有資訊的 json\n",
    "\n",
    "#開起每天的url檔案\n",
    "f = open('C:/Users/ytchen/Desktop/AppleNews/{}.txt'.format(file_datestring),'r')\n",
    "for url_no in f.readlines():\n",
    "#     print(url_no)\n",
    "    try:\n",
    "        category = url_no.split('http')[0]        #把類別抓出來\n",
    "        url = 'http'+url_no.split('http')[1]      #重新組合url\n",
    "        res = requests.get(url) #利用get發送請求\n",
    "        soup = BeautifulSoup(res.text)\n",
    "        file_date = file_datestring #日期當作檔案的名子\n",
    "#         print(file_date)\n",
    "        \n",
    "        #抓標題跟副標題\n",
    "        title = soup.select('#h1')[0].text\n",
    "#         title2 = soup.select('#h2')[0].text            \n",
    "#         title =''\n",
    "#         titles = title1 + title2\n",
    "#         title = '-'.join(titles.split())\n",
    "\n",
    "        \n",
    "        #抓人氣\n",
    "        click = soup.select('.function_icon.clicked')[0].text\n",
    "        click = click.split('(')[1].replace(')','')\n",
    "        \n",
    "        #抓內文\n",
    "        article = ''\n",
    "        div= soup.select('.articulum')[0]   #先把所有內文抓到\n",
    "        ps = div.select('p')                #再把裡面所有的p標籤抓到\n",
    "        for p in ps:                        #把每個p標籤一個一個抓出來印\n",
    "            article += p.text.replace('\\r','').replace('\\n','')  #段落會隱藏換行符號\\r，要取代掉，不然後面讀取json格式會只讀到最後一行\n",
    "        \n",
    "        #將資訊存成 json檔\n",
    "        data  =  { \n",
    "            'comp':'apple',  #新聞來源\n",
    "            'category':category, #類別\n",
    "            'title':title, #標題\n",
    "            'content':article, #內文\n",
    "            'date':file_date,  #直接抓檔案的日期來用\n",
    "            'url':url,\n",
    "            'keyw':'',\n",
    "            'click':click\n",
    "        }\n",
    "        \n",
    "#         print(title)\n",
    "        #一篇新聞一本字典\n",
    "        all_json_str.append(data) \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    except:\n",
    "        error_count +=1\n",
    "        error_url += url+','\n",
    "        continue \n",
    "        \n",
    "            \n",
    "# #檔案儲存\n",
    "# f1 = open('C:/Users/ytchen/Desktop/AppleNews/json/{}.json'.format(file_datestring),'w')\n",
    "# b =json.dumps(all_json_str) #再把 list 字典轉換成 json 並包起來\n",
    "# f1.write(b)\n",
    "\n",
    "#寫入檔案結束\n",
    "f1.close()\n",
    "f.close()\n",
    "\n",
    "\n",
    "#log記錄\n",
    "logString = ''\n",
    "fileName_log = str(file_datestring)+'/'+'errorCount:'+str(error_count)+'\\n'+'error_url:'+error_url+'\\n'\n",
    "#讀取\n",
    "with open('C:/Users/ytchen/Desktop/AppleNews/log_content.txt','r') as f1:\n",
    "    for logLine in f1.readlines():\n",
    "        logString += logLine\n",
    "#重新寫入\n",
    "with open('C:/Users/ytchen/Desktop/AppleNews/log_content.txt','w') as f2:\n",
    "    logString += fileName_log\n",
    "    f2.write(logString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將檔案寫入mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import datetime\n",
    "today = datetime.datetime.now()\n",
    "yestoday = today + datetime.timedelta(days = -1)\n",
    "yestoday = yestoday.strftime('%Y%m%d')\n",
    "file_datestring = yestoday\n",
    "\n",
    "f = open('C:/Users/ytchen/Desktop/AppleNews/json/{}.json'.format(file_datestring),'r')\n",
    "i = f.read()\n",
    "json = json.loads(i)\n",
    "print(len(json))\n",
    "    \n",
    "client = MongoClient() \n",
    "database = client[\"test\"]\n",
    "collection = database[\"news\"]\n",
    "collection.insert_many(\n",
    "    json\n",
    ")\n",
    "f.close()\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
